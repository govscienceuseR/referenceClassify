---
title: "referenceClassify"
subtitle: "A vignette using California's Groundwater Sustainability Plan documents"
output: html_document
---
<!----
These were in the yaml but the html won't render to the vignette folder and that is annoying to me, so instead i run this: rmarkdown::render('vignettes/sgma.Rmd', output_dir = 'vignettes')
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sgma}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
--->
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = F
)
```

The referenceClassify tool from govscienceuseR is designed to take a data frame of tagged reference data (author, year, container, publisher, doi, etc), run exact matching to various reference indices, and further classify potential references into categories. We walk through these steps using California's Groundwater Sustainability Plan PDF documents. Check out the [vignette on the referenceExtract](http://htmlpreview.github.io/?https://github.com/govscienceuseR/referenceExtract/blob/master/vignettes/sgma.html) tool see the steps that prepared the data we use in this vignette.  

First, be sure to download the referenceClassify package from github (`devtools::install_github("govscienceuseR/referenceClassify")` and load it in, as below. Also load the packages listed below.  

```{r setup}
library(referenceClassify)
```

We will be analyzing potential references extracted from Groundwater Sustainability Plan documents, previewed below.  
```{r}
dt <- fread("~/Box/reference_classifier/data/gsp_references_clean.csv")
head(as_tibble(select(dt, -c(File, ID))), n = 10)
```

Based on our initial cleaning in the referenceExtract package, we can look to see which of these potential references are exact matches to three different indices: an index of academic journals from the Scimago database, an index of academic conference papers/proceedings also from the Scimago database, and an index of US state and federal agencies, curated by the packages authors. We can look for matches using the `journal_match()`, `conference_match()` and `agency_match()` functions.
```{r}
table(journal_match(dt$container))
```

```{r}
table(conference_match(dt$container))
```

```{r}
table(agency_match(dt$container, dt$author))
```

After this initial assessment, we can further clean and refine these potential references with the `prepared_by` function, which removes commonly-seen lead-ins to references ('prepared for/by', etc.) to improve exact matching.   
```{r prepared by}
dt <- prepared_by(dt, x = 'container', y = 'author', z = 'publisher')
```

Next, we disambiguate the journals with the `journal_disambig` function, which references indices of common journal abbreviations and through manual cleaning of journals referenced in transportation documents. We choose to assign this to a new variable, journal_disam.
```{r journal disam}
dt$journal_disam <- journal_disambig(dt$container)
```


Third, we use regular expressions to look at our cleaned data and classify based on exact matches with regular expressions using the `regex_classify()` function. This function does two things. First it looks across all of the columns for exact matches to our indices, and if there is an exact match, it pulls out that value into a 'input' column. If there is not exact match, the value in the input column will be selected in the following order of preference: doi, container, publisher, author, title. Second, based on the matches the function will assign the potential reference into one of four classes: journal, agency, conference, or none. If none of the potential references' data is an exact match to any of the indices, the classification is NA. 
```{r regex classify}
dt <- regex_classify(dt, 'journal_disam')
table(dt$class)
```


```{r keras classify}
setwd("~/")
dt_classified <- filter(dt, !is.na(class)) %>% 
  mutate(classification = "regex")
dt_toclassify <- filter(dt, is.na(class))
# Something is wrong with the auto_input
# And something is wrong with the path 
predictions <- keras_classify(dt_toclassify, probability = .9, 
                              'journal_disam', auto_input = F, 'training_input')
dt_toclassify <- cbind(select(dt_toclassify, -class), select(predictions, predict_class)) %>% 
  rename("class" = "predict_class") %>% 
  mutate(classification = "keras")
dt <- rbind(dt_classified, dt_toclassify)
```

```{r}
write.csv(dt, "~/Documents/Davis/R-Projects/referenceClassify/data/predicted_class.csv", row.names = F)
```

```{r, eval = F}
table(dt_toclassify$predict_class)
```

```{r, eval = F}
head(as_tibble(select(filter(dt_toclassify, predict_class == "agency"),
                      author, year, title, container, publisher)), n = 10)
```

```{r, eval = F}
head(as_tibble(select(filter(dt_toclassify, predict_class == "journal"),
                      author, year, title, container, publisher, doi)), n = 10)
```

```{r, eval = F}
head(as_tibble(select(filter(dt_toclassify, predict_class == "delete"),
                      author, year, title, container, publisher)), n = 10)
```

